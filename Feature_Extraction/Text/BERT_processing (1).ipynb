{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28458,"status":"ok","timestamp":1711904347930,"user":{"displayName":"Javier Torralba Flores","userId":"04030390242553285581"},"user_tz":-180},"id":"6k7s7Ts2_jiU","outputId":"2b38a4c0-d76a-4f03-b2ed-2e81e919800f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8764,"status":"ok","timestamp":1711706071880,"user":{"displayName":"Javier Rene Torralba Flores","userId":"10057736132246643076"},"user_tz":-120},"id":"s5SXdJvYOdDi","outputId":"78b17e32-b5c6-46d6-c010-155c2eb8c170"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":116801,"status":"ok","timestamp":1711743887068,"user":{"displayName":"Javier Rene Torralba Flores","userId":"10057736132246643076"},"user_tz":-60},"id":"vlT_hQoDFxya","outputId":"e98f93d0-e8e3-4635-f1f9-0872f2ed8bdd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pytorch_pretrained_bert\n","  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/123.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m122.9/123.8 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pytorch-nlp\n","  Downloading pytorch_nlp-0.5.0-py3-none-any.whl (90 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.1/90.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2.2.1+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (1.25.2)\n","Collecting boto3 (from pytorch_pretrained_bert)\n","  Downloading boto3-1.34.74-py3-none-any.whl (139 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2.31.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (4.66.2)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from pytorch_pretrained_bert) (2023.12.25)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.13.3)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.10.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=0.4.1->pytorch_pretrained_bert)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=0.4.1->pytorch_pretrained_bert)\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=0.4.1->pytorch_pretrained_bert)\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=0.4.1->pytorch_pretrained_bert)\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=0.4.1->pytorch_pretrained_bert)\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=0.4.1->pytorch_pretrained_bert)\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=0.4.1->pytorch_pretrained_bert)\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=0.4.1->pytorch_pretrained_bert)\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=0.4.1->pytorch_pretrained_bert)\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=0.4.1->pytorch_pretrained_bert)\n","  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=0.4.1->pytorch_pretrained_bert)\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=0.4.1->pytorch_pretrained_bert)\n","  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting botocore<1.35.0,>=1.34.74 (from boto3->pytorch_pretrained_bert)\n","  Downloading botocore-1.34.74-py3-none-any.whl (12.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->pytorch_pretrained_bert)\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->pytorch_pretrained_bert)\n","  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pytorch_pretrained_bert) (2024.2.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.74->boto3->pytorch_pretrained_bert) (2.8.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.1->pytorch_pretrained_bert) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.1->pytorch_pretrained_bert) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.74->boto3->pytorch_pretrained_bert) (1.16.0)\n","Installing collected packages: pytorch-nlp, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, jmespath, nvidia-cusparse-cu12, nvidia-cudnn-cu12, botocore, s3transfer, nvidia-cusolver-cu12, boto3, pytorch_pretrained_bert\n","Successfully installed boto3-1.34.74 botocore-1.34.74 jmespath-1.0.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 pytorch-nlp-0.5.0 pytorch_pretrained_bert-0.6.2 s3transfer-0.10.1\n"]}],"source":["!pip install pytorch_pretrained_bert pytorch-nlp"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3920,"status":"ok","timestamp":1711905876991,"user":{"displayName":"Javier Torralba Flores","userId":"04030390242553285581"},"user_tz":-180},"id":"gvmDPI7uQjfh"},"outputs":[],"source":["import pandas as pd\n","\n","# Load the data!\n","data = pd.read_csv('/content/drive/MyDrive/MSc Thesis Data Science/influencer_brands/Data/Metadata/all_metadata_5.csv', lineterminator='\\n')"]},{"cell_type":"markdown","metadata":{"id":"182623ozSP1J"},"source":["Step 1: Turn the captions into a list and pre process captions"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":966,"status":"ok","timestamp":1711905879247,"user":{"displayName":"Javier Torralba Flores","userId":"04030390242553285581"},"user_tz":-180},"id":"c8G8j4giSPTJ"},"outputs":[],"source":["# Group by 'post_id' and select the first caption\n","data = data.groupby('post_id').first().reset_index()\n","\n","# Convert the captions column to a list\n","captions = data['caption'].tolist()"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":382,"status":"ok","timestamp":1711905881235,"user":{"displayName":"Javier Torralba Flores","userId":"04030390242553285581"},"user_tz":-180},"id":"pssNU9zZOHf0"},"outputs":[],"source":["# Get posts ID index\n","post_ids = data['post_id'].tolist()"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1711905883017,"user":{"displayName":"Javier Torralba Flores","userId":"04030390242553285581"},"user_tz":-180},"id":"eWqYKbsdCmgP"},"outputs":[],"source":["# Convert all captions to strings, handling NaN values\n","captions = data['caption'].fillna('').astype(str)"]},{"cell_type":"markdown","metadata":{"id":"gFr6Es0lRuOY"},"source":["Step 2: Define the tokenizers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WE8JoXqN5qXp"},"outputs":[],"source":[" # Add special tokens at the beginning and end of each sentence for BERT to work properly\n","#captions = [\" [CLS] \" + caption + \" [SEP]\" for caption in captions]"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":110766,"status":"ok","timestamp":1711905995983,"user":{"displayName":"Javier Torralba Flores","userId":"04030390242553285581"},"user_tz":-180},"id":"nWNOwi0XQuKE","outputId":"885c29fb-dbde-4517-8b7c-0fcfaac497c6"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Tokenize the first sentence:\n","['Your', 'Ġda', 'is', 'ies', 'Ġare', 'Ġnumbered', 'Ġ@', 'v', 'ail', 'mt', 'n', '.', 'ĠWinter', 'Ġis', 'Ġcoming', '.', 'Ġ#', 'p', 'ushing', 'da', 'is', 'ies', 'Ġ#', 'v', 'ail', 'Ġ#', 'v', 'ail', 'mt', 'n', 'Ġ#', 'v', 'ails', 'ummer', 'Ġ#', 'color', 'ado', 'Ġ#', 'mount', 'ain', 'Ġ#', 'fl', 'owers']\n"]}],"source":["#from transformers import RobertaTokenizer, RobertaModel\n","#from pytorch_pretrained_bert import BertTokenizer, BertConfig\n","#from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n","from transformers import RobertaTokenizer\n","\n","#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","tokenizer = RobertaTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base')\n","\n","\n","tokenized_texts = [tokenizer.tokenize(caption) for caption in captions]\n","\n","\n","print(\"Tokenize the first sentence:\")\n","print(tokenized_texts[69])\n","#\n","#model = RobertaModel.from_pretrained('cardiffnlp/twitter-roberta-base')"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":564},"executionInfo":{"elapsed":1729,"status":"ok","timestamp":1711905997710,"user":{"displayName":"Javier Torralba Flores","userId":"04030390242553285581"},"user_tz":-180},"id":"IP3Gm6US9In-","outputId":"0258f4f9-9d9d-4ef8-efe7-f87eb7e05601"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x600 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["import matplotlib.pyplot as plt\n","\n","# Calculate the length of each tokenized text\n","lengths = [len(tokens) for tokens in tokenized_texts]\n","\n","# Plot a histogram of the lengths\n","plt.figure(figsize=(10,6))\n","plt.hist(lengths, bins=50)\n","plt.title(\"Distribution of Caption Lengths\")\n","plt.xlabel(\"Length of Tokenized Text\")\n","plt.ylabel(\"Number of Captions\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"JqUOPZYIJ5cw"},"source":["Step 3: Convert Tokens to IDs"]},{"cell_type":"markdown","metadata":{"id":"r_WJMnJWNcID"},"source":["Note: I use truncation here but maybe look into alternatives because of loss of data."]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":21372,"status":"ok","timestamp":1711906019080,"user":{"displayName":"Javier Torralba Flores","userId":"04030390242553285581"},"user_tz":-180},"id":"C3qcQRegOAld"},"outputs":[],"source":["# Truncate or pad the tokenized text to a specific length\n","max_len = 270\n","input_ids = [tokenizer.convert_tokens_to_ids(x[:max_len]) for x in tokenized_texts]"]},{"cell_type":"markdown","metadata":{"id":"QEQbpwRyJ94x"},"source":["Step 4: Add Padding"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":7828,"status":"ok","timestamp":1711906026893,"user":{"displayName":"Javier Torralba Flores","userId":"04030390242553285581"},"user_tz":-180},"id":"jI05UZ4jHXu8"},"outputs":[],"source":["from keras.preprocessing.sequence import pad_sequences\n","\n","MAX_LEN = 270\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"]},{"cell_type":"markdown","metadata":{"id":"kTqf-ZbiPfB0"},"source":["Step 5: Create attention masks for padding and non padding"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":16327,"status":"ok","timestamp":1711906043204,"user":{"displayName":"Javier Torralba Flores","userId":"04030390242553285581"},"user_tz":-180},"id":"ds09KyspPetq"},"outputs":[],"source":["attention_masks = [[float(i > 0) for i in seq] for seq in input_ids]"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2342,"status":"ok","timestamp":1711906045530,"user":{"displayName":"Javier Torralba Flores","userId":"04030390242553285581"},"user_tz":-180},"id":"WS1TEWsXbBAq","outputId":"1d712b7e-71b0-4e9e-f5a6-570eae2c49c6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":10}],"source":["# Deletting variables\n","import gc\n","\n","# Delete variables you don't need anymore\n","del max_len, MAX_LEN, tokenized_texts, captions, data, tokenizer\n","\n","# Run garbage collector\n","gc.collect()"]},{"cell_type":"markdown","metadata":{"id":"Z8aSxKtNP_oP"},"source":["Step 6: Extract features"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"OfBc06tgwu0O","executionInfo":{"status":"error","timestamp":1711908684607,"user_tz":-180,"elapsed":2639080,"user":{"displayName":"Javier Torralba Flores","userId":"04030390242553285581"}},"outputId":"65d9b6a6-4a37-4667-f271-075bb5a7e72c"},"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-66c06faf5794>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Process each batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# hidden_states is a tuple of 13 elements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    833\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         )\n\u001b[0;32m--> 835\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    836\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    522\u001b[0m                 )\n\u001b[1;32m    523\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    525\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    414\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 340\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    341\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0mmixed_query_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;31m# If this is instantiated as a cross-attention module, the keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torch\n","from transformers import RobertaModel\n","import numpy as np\n","import gc\n","\n","# Convert to torch tensors\n","input_ids = torch.tensor(input_ids)\n","attention_masks = torch.tensor(attention_masks)\n","\n","# Load pre-trained model\n","model = RobertaModel.from_pretrained('cardiffnlp/twitter-roberta-base', output_hidden_states=True)\n","\n","# Put the model in evaluation mode\n","model.eval()\n","\n","# Define batch size\n","batch_size = 250\n","\n","# Calculate the number of batches\n","num_batches = len(input_ids) // batch_size + (0 if len(input_ids) % batch_size == 0 else 1)\n","\n","# Specify the starting batch number (modify this as needed)\n","start_from_batch = 529  # Replace 60 with the batch number you want to start from\n","\n","for batch_num in range(start_from_batch, num_batches):\n","    start_index = batch_num * batch_size\n","    end_index = min((batch_num + 1) * batch_size, len(input_ids))\n","    batch_post_ids = post_ids[start_index:end_index]  # Get corresponding post IDs for the batch\n","\n","    # Process each batch\n","    with torch.no_grad():\n","        outputs = model(input_ids[start_index:end_index], attention_mask=attention_masks[start_index:end_index])\n","        hidden_states = outputs[2]  # hidden_states is a tuple of 13 elements\n","\n","        # Use only the last layer's output and calculate mean across all tokens in each caption\n","        last_layer = hidden_states[-1]\n","        mean_last_layer = torch.mean(last_layer, dim=1)\n","\n","        # Convert the mean tensor to a NumPy array and save it for this batch\n","        np.save(f'/content/drive/MyDrive/Colab Notebooks/MSc thesis/processed_data/text/caption_features_mean_batch_{batch_num}.npy', mean_last_layer.numpy())\n","        np.save(f'/content/drive/MyDrive/Colab Notebooks/MSc thesis/processed_data/text/post_ids_batch_{batch_num}.npy', np.array(batch_post_ids))\n","\n","    # Clear memory\n","    del outputs, hidden_states, last_layer, mean_last_layer\n","    gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":405,"referenced_widgets":["31b87e381b75432388429010d1af2214","cfc7d83389ab419bab677dc8319ff156","4d9adb428b60414aaff6f9e845dd31ba","acc55836c5754c0e9b4a9b307e909a50","f2bb070bdc104da0a3fee032a5106a2b","b7523446757a420fb594e0caf1c290d7","1426744e41ae45339ea2b2d18b40554d","4c178279ddaa449e90488c9ea131c586","02017c07b2614be18e1516707cd4fef8","244ec2276bb249ef933a19200b9ba7c3","1e291f8906f445568c8841de526b87d9"]},"executionInfo":{"elapsed":317314,"status":"error","timestamp":1711708173095,"user":{"displayName":"Javier Rene Torralba Flores","userId":"10057736132246643076"},"user_tz":-120},"id":"uZogZmfNelyk","outputId":"08f28787-487c-4b9e-b7c8-32c845dd1a2d"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"31b87e381b75432388429010d1af2214","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/501M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-c819f17409e8>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Process each batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# hidden_states is a tuple of 13 elements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    833\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         )\n\u001b[0;32m--> 835\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    836\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    522\u001b[0m                 )\n\u001b[1;32m    523\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    525\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    456\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torch\n","#from transformers import BertModel\n","from transformers import RobertaModel\n","import numpy as np\n","import gc\n","\n","# Convert to torch tensors\n","input_ids = torch.tensor(input_ids)\n","attention_masks = torch.tensor(attention_masks)\n","\n","# Load pre-trained model\n","#model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n","model = RobertaModel.from_pretrained('cardiffnlp/twitter-roberta-base', output_hidden_states=True)\n","\n","# Put the model in evaluation mode to deactivate dropout layers\n","model.eval()\n","\n","# Define batch size\n","batch_size = 250  # Example batch size\n","\n","# Calculate the number of batches\n","num_batches = len(input_ids) // batch_size + (0 if len(input_ids) % batch_size == 0 else 1)\n","\n","for batch_num in range(num_batches):\n","    start_index = batch_num * batch_size\n","    end_index = min((batch_num + 1) * batch_size, len(input_ids))\n","    batch_post_ids = post_ids[start_index:end_index]  # Get corresponding post IDs for the batch\n","\n","    # Process each batch\n","    with torch.no_grad():\n","        outputs = model(input_ids[start_index:end_index], attention_mask=attention_masks[start_index:end_index])\n","        hidden_states = outputs[2]  # hidden_states is a tuple of 13 elements\n","\n","        # Use only the last layer's output and calculate mean across all tokens in each caption\n","        last_layer = hidden_states[-1]\n","        mean_last_layer = torch.mean(last_layer, dim=1)\n","\n","        # Convert the mean tensor to a NumPy array and save it for this batch\n","        np.save(f'/content/drive/MyDrive/Colab Notebooks/MSc thesis/processed_data/text/caption_features_mean_batch_{batch_num}.npy', mean_last_layer.numpy())\n","        np.save(f'/content/drive/MyDrive/Colab Notebooks/MSc thesis/processed_data/text/post_ids_batch_{batch_num}.npy', np.array(batch_post_ids))\n","\n","    # Clear memory\n","    del outputs, hidden_states, last_layer, mean_last_layer\n","    gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":532},"executionInfo":{"elapsed":347437,"status":"error","timestamp":1711619459879,"user":{"displayName":"Javier Torralba Flores","userId":"04030390242553285581"},"user_tz":-120},"id":"qd-TFHAHV-hS","outputId":"df2779f4-a501-4bda-b0dd-0c5cb9784759"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-5affae52bd4a>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Process each batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# hidden_states is a tuple of 13 elements\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1011\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         )\n\u001b[0;32m-> 1013\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1014\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    605\u001b[0m                 )\n\u001b[1;32m    606\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    608\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    498\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 427\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    428\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;31m# Normalize the attention scores to probabilities.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0mattention_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;31m# This is actually dropping out entire tokens to attend to, which might\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1828\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1829\u001b[0m     r\"\"\"Apply a softmax function.\n\u001b[1;32m   1830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["import torch\n","from transformers import BertModel\n","import numpy as np\n","import gc\n","\n","# Convert to torch tensors\n","input_ids = torch.tensor(input_ids)\n","attention_masks = torch.tensor(attention_masks)\n","\n","# Load pre-trained model\n","model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n","\n","# Put the model in evaluation mode to deactivate dropout layers\n","model.eval()\n","\n","# Define batch size (adjust based on your available memory)\n","batch_size = 100  # Example batch size\n","\n","# Calculate the number of batches\n","num_batches = len(input_ids) // batch_size + (0 if len(input_ids) % batch_size == 0 else 1)\n","\n","for batch_num in range(num_batches):\n","    start_index = batch_num * batch_size\n","    end_index = min((batch_num + 1) * batch_size, len(input_ids))\n","\n","    # Process each batch\n","    with torch.no_grad():\n","        outputs = model(input_ids[start_index:end_index], attention_mask=attention_masks[start_index:end_index])\n","        hidden_states = outputs[2]  # hidden_states is a tuple of 13 elements\n","\n","        concatenated_hidden_states = []\n","\n","        for i in range(len(outputs[0])):\n","            token_vecs_cat = [torch.cat((hidden_states[-1][i][token],\n","                                         hidden_states[-2][i][token],\n","                                         hidden_states[-3][i][token],\n","                                         hidden_states[-4][i][token]), dim=0)\n","                              for token in range(len(input_ids[start_index + i]))]\n","            concatenated_hidden_states.append(token_vecs_cat)\n","\n","        # Convert the list of concatenated hidden states to a tensor for this batch\n","        concatenated_hidden_states_tensor = torch.stack([torch.stack(seq) for seq in concatenated_hidden_states])\n","\n","        # Convert the tensor to a NumPy array and save it for this batch\n","        np.save(f'/content/drive/MyDrive/Colab Notebooks/MSc thesis/processed_data/text/caption_features_batch_{batch_num}.npy', concatenated_hidden_states_tensor.numpy())\n","\n","    # Clear memory\n","    del outputs, hidden_states, concatenated_hidden_states, concatenated_hidden_states_tensor\n","    gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":205,"referenced_widgets":["5aa8ee5822c7427abfbd7a48ae797da2","8a0090936cf1480e87be0ba509bb29bb","e03f9e2516d64cd293c145e4c87b6316","3a4d0b9334424e61977cd70cfe41b83d","415c95e1908b4c03a276a8cd89e3c4e7","6e2c813d392e40b3a1191d8204384267","482117f45a7d4a11bd9fe262b5e1502d","9d9fe24579b943f0b4b05a670b3f3d66","f423571d95f142f1a0a7fc3ffbca9aab","e629e29a07a7434bb7e9d63b93782b75","6a386f25611545c48c40a9337ec8d99f","64483c40608d47bf92daccd9138c6559","02f2c0ca6b2c4947806d1b8e2540b396","5716177c5fe04fc6901da62e97fd4fcd","c5479903c91741f79988ed59c2c3b4b3","ce67d17ee0fe41aaa1306638bb704eab","0c6f9294d46e46a9bab90e65138520f0","c094aadb399b4c70aa809d01242933cb","4c526e9815ef4c2496e142bd60ed7eec","182e3ce747354579bf8ab336922ab988","27062736553b4d5a9279204f33f3314f","0563f70375e945df8c4e15718ca8676b"]},"id":"B6yVr454SMDg","outputId":"3d2d0638-8c78-48a5-9716-6c8bc9e5490d"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5aa8ee5822c7427abfbd7a48ae797da2","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"64483c40608d47bf92daccd9138c6559","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import torch\n","from transformers import BertModel\n","\n","# Convert to torch tensors\n","input_ids = torch.tensor(input_ids)\n","attention_masks = torch.tensor(attention_masks)\n","\n","# Load pre-trained model\n","model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)\n","\n","# Put the model in evaluation mode to deactivate dropout layers\n","model.eval()\n","\n","# Initialize the list to hold the concatenated hidden states for each token\n","concatenated_hidden_states = []\n","\n","# Run the text through BERT, and collect all of the hidden states produced from all 12 layers.\n","with torch.no_grad():\n","    outputs = model(input_ids, attention_mask=attention_masks)\n","    hidden_states = outputs[2]  # hidden_states is a tuple of 13 elements (1 for the embedding layer and 12 for the transformer layers)\n","\n","    # Concatenate the last four layers for each token of each sequence\n","    for i in range(input_ids.size(0)):  # loop over each sequence\n","        token_vecs_cat = []\n","\n","        for token in range(input_ids.size(1)):  # loop over each token in the sequence\n","            # Concatenate the vectors (hidden states) from the last four layers\n","            # `hidden_states` is a tuple, and we are interested in its last four elements\n","            cat_vec = torch.cat(\n","                (hidden_states[-1][i][token],\n","                 hidden_states[-2][i][token],\n","                 hidden_states[-3][i][token],\n","                 hidden_states[-4][i][token]), dim=0)\n","            token_vecs_cat.append(cat_vec)\n","\n","        concatenated_hidden_states.append(token_vecs_cat)\n","\n","# Now `concatenated_hidden_states` is a list where each element is a list of concatenated token vectors for each input sequence"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OPscP6NtSkvi"},"outputs":[],"source":["import numpy as np\n","\n","# Convert the list of concatenated hidden states to a tensor\n","concatenated_hidden_states_tensor = torch.stack([torch.stack(seq) for seq in concatenated_hidden_states])\n","\n","# Convert the tensor to a NumPy array and save it\n","np.save('/content/drive/MyDrive/Colab Notebooks/MSc thesis/processed_data/text/caption_features.npy', concatenated_hidden_states_tensor.numpy())"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"02017c07b2614be18e1516707cd4fef8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"02f2c0ca6b2c4947806d1b8e2540b396":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c6f9294d46e46a9bab90e65138520f0","placeholder":"​","style":"IPY_MODEL_c094aadb399b4c70aa809d01242933cb","value":"model.safetensors: 100%"}},"0563f70375e945df8c4e15718ca8676b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0c6f9294d46e46a9bab90e65138520f0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1426744e41ae45339ea2b2d18b40554d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"182e3ce747354579bf8ab336922ab988":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1e291f8906f445568c8841de526b87d9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"244ec2276bb249ef933a19200b9ba7c3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"27062736553b4d5a9279204f33f3314f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31b87e381b75432388429010d1af2214":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cfc7d83389ab419bab677dc8319ff156","IPY_MODEL_4d9adb428b60414aaff6f9e845dd31ba","IPY_MODEL_acc55836c5754c0e9b4a9b307e909a50"],"layout":"IPY_MODEL_f2bb070bdc104da0a3fee032a5106a2b"}},"3a4d0b9334424e61977cd70cfe41b83d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e629e29a07a7434bb7e9d63b93782b75","placeholder":"​","style":"IPY_MODEL_6a386f25611545c48c40a9337ec8d99f","value":" 570/570 [00:00&lt;00:00, 37.7kB/s]"}},"415c95e1908b4c03a276a8cd89e3c4e7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"482117f45a7d4a11bd9fe262b5e1502d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4c178279ddaa449e90488c9ea131c586":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4c526e9815ef4c2496e142bd60ed7eec":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4d9adb428b60414aaff6f9e845dd31ba":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c178279ddaa449e90488c9ea131c586","max":501204462,"min":0,"orientation":"horizontal","style":"IPY_MODEL_02017c07b2614be18e1516707cd4fef8","value":501204462}},"5716177c5fe04fc6901da62e97fd4fcd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c526e9815ef4c2496e142bd60ed7eec","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_182e3ce747354579bf8ab336922ab988","value":440449768}},"5aa8ee5822c7427abfbd7a48ae797da2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8a0090936cf1480e87be0ba509bb29bb","IPY_MODEL_e03f9e2516d64cd293c145e4c87b6316","IPY_MODEL_3a4d0b9334424e61977cd70cfe41b83d"],"layout":"IPY_MODEL_415c95e1908b4c03a276a8cd89e3c4e7"}},"64483c40608d47bf92daccd9138c6559":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_02f2c0ca6b2c4947806d1b8e2540b396","IPY_MODEL_5716177c5fe04fc6901da62e97fd4fcd","IPY_MODEL_c5479903c91741f79988ed59c2c3b4b3"],"layout":"IPY_MODEL_ce67d17ee0fe41aaa1306638bb704eab"}},"6a386f25611545c48c40a9337ec8d99f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6e2c813d392e40b3a1191d8204384267":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a0090936cf1480e87be0ba509bb29bb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e2c813d392e40b3a1191d8204384267","placeholder":"​","style":"IPY_MODEL_482117f45a7d4a11bd9fe262b5e1502d","value":"config.json: 100%"}},"9d9fe24579b943f0b4b05a670b3f3d66":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"acc55836c5754c0e9b4a9b307e909a50":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_244ec2276bb249ef933a19200b9ba7c3","placeholder":"​","style":"IPY_MODEL_1e291f8906f445568c8841de526b87d9","value":" 501M/501M [00:02&lt;00:00, 215MB/s]"}},"b7523446757a420fb594e0caf1c290d7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c094aadb399b4c70aa809d01242933cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c5479903c91741f79988ed59c2c3b4b3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_27062736553b4d5a9279204f33f3314f","placeholder":"​","style":"IPY_MODEL_0563f70375e945df8c4e15718ca8676b","value":" 440M/440M [00:04&lt;00:00, 35.8MB/s]"}},"ce67d17ee0fe41aaa1306638bb704eab":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cfc7d83389ab419bab677dc8319ff156":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7523446757a420fb594e0caf1c290d7","placeholder":"​","style":"IPY_MODEL_1426744e41ae45339ea2b2d18b40554d","value":"pytorch_model.bin: 100%"}},"e03f9e2516d64cd293c145e4c87b6316":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d9fe24579b943f0b4b05a670b3f3d66","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f423571d95f142f1a0a7fc3ffbca9aab","value":570}},"e629e29a07a7434bb7e9d63b93782b75":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2bb070bdc104da0a3fee032a5106a2b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f423571d95f142f1a0a7fc3ffbca9aab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":0}