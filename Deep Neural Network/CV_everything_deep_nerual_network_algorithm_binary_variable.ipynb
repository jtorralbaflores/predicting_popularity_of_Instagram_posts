{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 6358,
     "status": "ok",
     "timestamp": 1715009852996,
     "user": {
      "displayName": "Javier Rene Torralba Flores",
      "userId": "10057736132246643076"
     },
     "user_tz": -120
    },
    "id": "8asSrgzSOFnF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ds3n3EZDyhE3"
   },
   "source": [
    "Step 0: load our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 135380,
     "status": "ok",
     "timestamp": 1715009988366,
     "user": {
      "displayName": "Javier Rene Torralba Flores",
      "userId": "10057736132246643076"
     },
     "user_tz": -120
    },
    "id": "KVmUwQisygzx"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/downloaded_data.csv\", lineterminator=\"\\n\")\n",
    "print(\"Loaded dataset with shape:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "executionInfo": {
     "elapsed": 1327,
     "status": "ok",
     "timestamp": 1715009989675,
     "user": {
      "displayName": "Javier Rene Torralba Flores",
      "userId": "10057736132246643076"
     },
     "user_tz": -120
    },
    "id": "DzlGzoyl06EI",
    "outputId": "54d74ba4-f6c9-47f3-b9f0-a3b8c8d066bb"
   },
   "outputs": [],
   "source": [
    "# List of image and text feature columns to keep\n",
    "image_columns = [f'image_{i}' for i in range(0, 2049)]\n",
    "text_columns = [f'text_{i}' for i in range(0, 768)]\n",
    "\n",
    "# Additional columns you want to keep\n",
    "additional_columns_to_keep = ['like_count', 'comment_count', 'post_id', 'Followers', 'Username', 'Followees', 'Posts',\n",
    "                              'Month', 'Day of Week (String)', 'Hour', 'Language', 'Is_English', 'Image_Count', 'Sponsorship label', 'is_video']\n",
    "\n",
    "# Combine both lists of columns to keep\n",
    "columns_to_keep = additional_columns_to_keep + text_columns + image_columns\n",
    "\n",
    "# Drop all columns except those specified in columns_to_keep\n",
    "columns_to_drop = [col for col in data.columns if col not in columns_to_keep]\n",
    "\n",
    "# Drop the unwanted columns from the dataframe\n",
    "data_2 = data.drop(columns=columns_to_drop)\n",
    "\n",
    "\n",
    "# Maybe don't delete category and detected languages\n",
    "# Same with comments disabled and video\n",
    "# potential columns to add or not:\n",
    "#'comments_disabled',\n",
    "#'is_video'\n",
    "# Category\n",
    "data_2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 3000,
     "status": "ok",
     "timestamp": 1715009992673,
     "user": {
      "displayName": "Javier Rene Torralba Flores",
      "userId": "10057736132246643076"
     },
     "user_tz": -120
    },
    "id": "8xrU93WX8Nm6"
   },
   "outputs": [],
   "source": [
    "# Filter image count to 10 or less, which is what is realistic in the industry\n",
    "data_2 = data_2[data_2['Image_Count'] <= 1]\n",
    "\n",
    "# Columns to one-hot encode\n",
    "columns_to_encode = ['Month', 'Hour', 'Day of Week (String)']\n",
    "\n",
    "# Apply one-hot encoding\n",
    "data_2 = pd.get_dummies(data_2, columns=columns_to_encode)\n",
    "\n",
    "# Find the new one-hot encoded columns\n",
    "encoded_columns = data_2.columns[data_2.columns.str.startswith(tuple(columns_to_encode))]\n",
    "\n",
    "# Convert only the new one-hot encoded columns and boolean columns to int\n",
    "data_2[encoded_columns] = data_2[encoded_columns].astype(int)\n",
    "\n",
    "# Convert English column to integer\n",
    "data_2['Is_English'] = data_2['Is_English'].astype(int)\n",
    "data_2['is_video'] = data_2['is_video'].astype(int)\n",
    "data_2['Sponsorship label'] = data_2['Sponsorship label'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SH_ydYnLxbGg"
   },
   "source": [
    "Step 1: group and split the data by post_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdLC0xzFsuxE"
   },
   "source": [
    "Define a popularity variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1715009992673,
     "user": {
      "displayName": "Javier Rene Torralba Flores",
      "userId": "10057736132246643076"
     },
     "user_tz": -120
    },
    "id": "Thdyw-_OfpaX"
   },
   "outputs": [],
   "source": [
    "# Set the engagement rate variable\n",
    "data_2['popularity'] = (((data_2['like_count'] + data_2['comment_count'])/data_2['Followers']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DblzzoiQtU_l"
   },
   "source": [
    "Let's split the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 488548,
     "status": "ok",
     "timestamp": 1715010481219,
     "user": {
      "displayName": "Javier Rene Torralba Flores",
      "userId": "10057736132246643076"
     },
     "user_tz": -120
    },
    "id": "mxc2gHFb8Jva",
    "outputId": "71bba2fa-73d6-49de-e02e-9680ccf12a0d"
   },
   "outputs": [],
   "source": [
    "# Define the number of iterations and an array to store accuracies\n",
    "n_iterations = 3\n",
    "accuracies = []\n",
    "\n",
    "for seed in range(n_iterations):\n",
    "    print(f\"Iteration {seed+1}\")\n",
    "\n",
    "    # Split users into training and temporary sets with a new seed each time\n",
    "    train_users, temp_users = train_test_split(data_2['Username'].unique(), test_size=0.2, random_state=seed)\n",
    "    val_users, test_users = train_test_split(temp_users, test_size=0.5, random_state=seed)\n",
    "\n",
    "    # Assign posts to training, validation, and test sets\n",
    "    train_data = data_2[data_2['Username'].isin(train_users)]\n",
    "    val_data = data_2[data_2['Username'].isin(val_users)]\n",
    "    test_data = data_2[data_2['Username'].isin(test_users)]\n",
    "\n",
    "    # Normalize the selected features\n",
    "    columns_to_normalize = ['Followers', 'Followees', 'Posts']\n",
    "    scaler = StandardScaler().fit(train_data[columns_to_normalize])\n",
    "    train_data.loc[:, columns_to_normalize] = scaler.transform(train_data[columns_to_normalize])\n",
    "    val_data.loc[:, columns_to_normalize] = scaler.transform(val_data[columns_to_normalize])\n",
    "    test_data.loc[:, columns_to_normalize] = scaler.transform(test_data[columns_to_normalize])\n",
    "\n",
    "    # Calculate the median popularity rate for each dataset\n",
    "    median_popularity_rate_train = train_data['popularity'].quantile(0.5)\n",
    "    train_data.loc[:, 'popularity'] = (train_data['popularity'] > median_popularity_rate_train).astype(int)\n",
    "    median_popularity_rate_val = val_data['popularity'].quantile(0.5)\n",
    "    val_data.loc[:, 'popularity'] = (val_data['popularity'] > median_popularity_rate_val).astype(int)\n",
    "    median_popularity_rate_test = test_data['popularity'].quantile(0.5)\n",
    "    test_data.loc[:, 'popularity'] = (test_data['popularity'] > median_popularity_rate_test).astype(int)\n",
    "\n",
    "    # Prepare data for XGBoost\n",
    "    X_train = train_data.drop(['popularity', 'post_id', 'like_count', 'comment_count', 'Username', 'Image_Count', 'Followers'], axis=1)\n",
    "    y_train = train_data['popularity']\n",
    "    X_val = val_data.drop(['popularity', 'post_id', 'like_count', 'comment_count', 'Username', 'Image_Count', 'Followers'], axis=1)\n",
    "    y_val = val_data['popularity']\n",
    "    X_test = test_data.drop(['popularity', 'post_id', 'like_count', 'comment_count', 'Username', 'Image_Count', 'Followers'], axis=1)\n",
    "    y_test = test_data['popularity']\n",
    "\n",
    "    # Create the model\n",
    "    model_b = Sequential()\n",
    "\n",
    "    # Assume X_train is your training data loaded previously with correct dimensions\n",
    "    # Add new layers\n",
    "    model_b.add(Dense(64, input_shape=(X_train.shape[1],), activation='relu'))\n",
    "    model_b.add(Dropout(0.5))\n",
    "    model_b.add(Dense(32, activation='relu'))\n",
    "    model_b.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Initialize the Adam optimizer with a specific learning rate\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "    #Compiling the model\n",
    "    model_b.compile(optimizer= optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    # Set up early stopping\n",
    "    early_stopping = EarlyStopping(monitor = 'val_accuracy',\n",
    "                               patience = 20,\n",
    "                               restore_best_weights = True)\n",
    "    # Fit the model\n",
    "    history = model_b.fit(X_train,\n",
    "                      y_train,\n",
    "                      epochs=500,\n",
    "                      batch_size=128,\n",
    "                      validation_data = (X_val, y_val),\n",
    "                      callbacks=[early_stopping])\n",
    "\n",
    "    test_loss, test_acc = model_b.evaluate(X_test, y_test)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    y_pred_prob = model_b.predict(X_test)\n",
    "    y_pred_binary = (y_pred_prob > 0.5).astype(int)\n",
    "    accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    print(f\"Accuracy for iteration {seed+1}: {accuracy:.2f}\")\n",
    "\n",
    "# Calculate the average accuracy across all iterations\n",
    "average_accuracy = np.mean(accuracies)\n",
    "print(f\"Average Accuracy across {n_iterations} iterations: {average_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1715010481219,
     "user": {
      "displayName": "Javier Rene Torralba Flores",
      "userId": "10057736132246643076"
     },
     "user_tz": -120
    },
    "id": "0ymSZ-uF_-DT",
    "outputId": "96948edf-ad57-4392-8610-d96a85d6b0e8"
   },
   "outputs": [],
   "source": [
    "# Alternatively, using numpy to generate bootstrap samples and calculate the percentiles\n",
    "# Determine the size of the accuracies array\n",
    "n = len(accuracies)\n",
    "bootstrap_samples = np.random.choice(accuracies, (10000, n), replace=True)\n",
    "ci_lower = np.percentile(bootstrap_samples, 2.5)\n",
    "ci_upper = np.percentile(bootstrap_samples, 97.5)\n",
    "\n",
    "print(f\"Bootstrap 95% Confidence Interval: {ci_lower:.5f} - {ci_upper:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 710
    },
    "executionInfo": {
     "elapsed": 3291,
     "status": "ok",
     "timestamp": 1715010484506,
     "user": {
      "displayName": "Javier Rene Torralba Flores",
      "userId": "10057736132246643076"
     },
     "user_tz": -120
    },
    "id": "jx1RcftEhpG9",
    "outputId": "489cfa5a-418e-4058-b442-d893a59290c4"
   },
   "outputs": [],
   "source": [
    "y_pred = model_b.predict(X_test)\n",
    "print(y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(y_pred, bins=30, kde=True)\n",
    "plt.title('Distribution of popularity, DNN')\n",
    "plt.xlabel('popularity')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 689
    },
    "executionInfo": {
     "elapsed": 2097,
     "status": "ok",
     "timestamp": 1715010486599,
     "user": {
      "displayName": "Javier Rene Torralba Flores",
      "userId": "10057736132246643076"
     },
     "user_tz": -120
    },
    "id": "Ou2ClI0pOb6j",
    "outputId": "62aebf44-55a3-4a91-e53f-829f6365649c"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, type_of_model):\n",
    "    # Predicting the probabilities for the test set\n",
    "    y_pred_prob = model.predict(X_test)\n",
    "    # Convert probabilities to binary labels\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "    # y_test should already be binary labels\n",
    "    y_true = y_test.astype(int)\n",
    "\n",
    "    # Generating the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Generating the classification report\n",
    "    clr = classification_report(y_true, y_pred, target_names=['Low', 'High'])\n",
    "\n",
    "    # Plotting the confusion matrix\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='viridis', xticklabels=['Low', 'High'], yticklabels=['Low', 'High'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n",
    "\n",
    "    # Print the classification report\n",
    "    print(clr)\n",
    "\n",
    "# Example usage\n",
    "evaluate_model(model_b, X_val, val_data['popularity'], 'LightGBM')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2451,
     "status": "ok",
     "timestamp": 1715009316948,
     "user": {
      "displayName": "Javier Rene Torralba Flores",
      "userId": "10057736132246643076"
     },
     "user_tz": -120
    },
    "id": "NJwofXK8HZrZ",
    "outputId": "fef1dfdb-58aa-4eb5-9eaa-c1f5444f9d84"
   },
   "outputs": [],
   "source": [
    "def evaluate_model_and_error_rates(model, X_test, y_test):\n",
    "    # Predicting the probabilities for the test set\n",
    "    y_pred_prob = model.predict(X_test).flatten()\n",
    "\n",
    "    # Convert probabilities to binary labels\n",
    "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "\n",
    "    # y_test should already be binary labels\n",
    "    y_true = y_test.astype(int)\n",
    "\n",
    "    # Generating the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Generating the classification report\n",
    "    clr = classification_report(y_true, y_pred, target_names=['Low', 'High'])\n",
    "\n",
    "    # Plotting the confusion matrix\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='viridis', xticklabels=['Low', 'High'], yticklabels=['Low', 'High'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print the classification report\n",
    "    print(clr)\n",
    "\n",
    "    # Calculate error rates by probability bins\n",
    "    results = pd.DataFrame({\n",
    "        'y_true': y_true,\n",
    "        'y_pred_prob': y_pred_prob\n",
    "    })\n",
    "\n",
    "    results['prob_bin'] = pd.cut(results['y_pred_prob'], bins=np.arange(0, 1.1, 0.1), labels=np.arange(0.05, 1.05, 0.1))\n",
    "\n",
    "    # Calculate error rates\n",
    "    error_rates = results.groupby('prob_bin').apply(\n",
    "        lambda x: np.mean(x['y_true'] != (x['y_pred_prob'] > 0.5))\n",
    "    )\n",
    "\n",
    "    # Plotting error rates by predicted probability bins\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    error_rates.plot(kind='bar', color='skyblue')\n",
    "    plt.title('Error Rates by Predicted Probability Bins')\n",
    "    plt.xlabel('Predicted Probability Bins')\n",
    "    plt.ylabel('Error Rate')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "evaluate_model_and_error_rates(model_b, X_test, test_data['popularity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_9MBJUMljMP"
   },
   "source": [
    "Let's add some SHAP values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1715011135612,
     "user": {
      "displayName": "Javier Rene Torralba Flores",
      "userId": "10057736132246643076"
     },
     "user_tz": -120
    },
    "id": "Q0mvA9IolmF_"
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Assuming the model is trained and X_test, X_train are defined\n",
    "background = X_train.iloc[:100].values  # using 100 sample data points as background\n",
    "\n",
    "# Initialize the SHAP DeepExplainer\n",
    "explainer = shap.DeepExplainer(model_b, background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "executionInfo": {
     "elapsed": 247,
     "status": "error",
     "timestamp": 1715011144282,
     "user": {
      "displayName": "Javier Rene Torralba Flores",
      "userId": "10057736132246643076"
     },
     "user_tz": -120
    },
    "id": "R6e0If4FoNaC",
    "outputId": "d666acdd-eca0-40c7-e8dc-0673c1ec646f"
   },
   "outputs": [],
   "source": [
    "# Calculate SHAP values\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# SHAP Force Plot for the first prediction\n",
    "shap.initjs()\n",
    "fig, ax = plt.subplots()\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0][0], X_test.iloc[0], matplotlib=True, show=False)\n",
    "plt.close(fig)\n",
    "\n",
    "# SHAP Summary Plot (Bar)\n",
    "shap.summary_plot(shap_values[0], X_test, plot_type=\"bar\", show=False)\n",
    "plt.close()\n",
    "\n",
    "# SHAP Summary Plot (Dot)\n",
    "shap.summary_plot(shap_values[0], X_test, show=False)\n",
    "plt.close()\n",
    "\n",
    "# SHAP Decision Plot for a subset of data points\n",
    "shap.decision_plot(explainer.expected_value[0], shap_values[0][:100], X_test.iloc[:100], show=False)\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
